{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3zkDxGwVzGV"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import itertools\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "     'This is the first-document.',\n",
        "     'This document is the second document.',\n",
        "     'And this is the third one.',\n",
        "     'Is this the first document?'\n",
        "]\n",
        "\n",
        "documents = [document.lower() for document in documents]\n",
        "documents = [re.sub('[^A-Za-z0-9\\ ]+', ' ', document) for document in documents]\n",
        "\n",
        "vocab = [document.split() for document in documents]\n",
        "vocab = list(set(list(itertools.chain(*vocab))))\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "print(f\"documents: {documents}\")\n",
        "print(f\"vocab: {vocab}\")\n",
        "print(f\"number of documents: {len(documents)} \\nvocab length: {len(vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJggmui5a2bR",
        "outputId": "720c59a1-fa37-4649-a8cb-5691a1c61435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "documents: ['this is the first document ', 'this document is the second document ', 'and this is the third one ', 'is this the first document ']\n",
            "vocab: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "number of documents: 4 \n",
            "vocab length: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Normalization\n",
        "Source : [machinelearningmastery](https://machinelearningmastery.com/vector-norms-machine-learning/)\n",
        "\n",
        "1.   The L1 norm that is calculated as the sum of the absolute values of the vector.\n",
        "2.   The L2 norm that is calculated as the square root of the sum of the squared vector values.\n",
        "3.   The max norm that is calculated as the maximum vector values.\n",
        "\n",
        "\n",
        "\n",
        "## TF-IDF\n",
        "Source : [medium](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089) by [William Scott](https://williamscott701.medium.com/)\n",
        "\n",
        "    TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)\n",
        "\n",
        "### Term Frequency\n",
        "This measures the frequency of a word in a document. This highly depends on the length of the document and the generality of the word, for example, a very common word such as “was” can appear multiple times in a document. But if we take two documents with 100 words and 10,000 words respectively, there is a high probability that the common word “was” is present more in the 10,000 worded document. But we cannot say that the longer document is more important than the shorter document. For this exact reason, we perform normalization on the frequency value, we divide the frequency with the total number of words in the document.\n",
        "\n",
        "Recall that we need to finally vectorize the document. When we plan to vectorize documents, we cannot just consider the words that are present in that particular document. If we do that, then the vector length will be different for both the documents, and it will not be feasible to compute the similarity. So, what we do is that we vectorize the documents on the vocab. Vocab is the list of all possible worlds in the corpus.\n",
        "\n",
        "We need the word counts of all the vocab words and the length of the document to compute TF. In case the term doesn’t exist in a particular document, that particular TF value will be 0 for that particular document. In an extreme case, if all the words in the document are the same, then TF will be 1. The final value of the normalised TF value will be in the range of [0 to 1]. 0, 1 inclusive.\n",
        "\n",
        "\n",
        "\n",
        "    tf(t,d) = count of t in d / number of words in d\n",
        "\n",
        "\n",
        "#### Document Frequency\n",
        "\n",
        "This measures the importance of documents in a whole set of the corpus. This is very similar to TF but the only difference is that TF is the frequency counter for a term t in document d, whereas DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term is present in the document at least once, we do not need to know the number of times the term is present.\n",
        "\n",
        "\n",
        "    df(t) = occurrence of t in N documents\n",
        "\n",
        "\n",
        "### Inverse Document Frequency\n",
        "\n",
        "IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because they are present in almost all of the documents, and N/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n",
        "\n",
        "    idf(t) = N/df\n",
        "\n",
        "Now there are few other problems with the IDF, when we have a large corpus size say N=10000, the IDF value explodes. So to dampen the effect we take the log of IDF.\n",
        "\n",
        "    idf(t) = log(N/(df + 1))\n",
        "\n",
        "\n",
        "    tf-idf(t, d) = tf(t, d) * log(N/(df + 1))\n"
      ],
      "metadata": {
        "id": "5ibmuJCdnnj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF without sklearn\n",
        "Note: This is not an efficient way of calculating tfidf vectors. This is just for demonstration purpose.\n"
      ],
      "metadata": {
        "id": "18pTaiGzbswk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(documents)\n",
        "vocab_lenght = len(vocab)\n",
        "token2id = {word:index for index, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "def normalize(X, norm='L2'):\n",
        "    if norm == \"L2\":\n",
        "        row_norm = np.sqrt((X * X).sum(axis=1))\n",
        "        X = (X.T / row_norm).T\n",
        "        return X\n",
        "\n",
        "tf_idf = np.zeros((N,vocab_lenght))\n",
        "tf_mat = np.zeros((N,vocab_lenght))\n",
        "for document_index, document in enumerate(documents):\n",
        "    tokens = document.split()\n",
        "    #print(f\"tokens: {tokens}\")\n",
        "    for token in tokens:\n",
        "        token_id = token2id[token]\n",
        "\n",
        "        tf = sum([1 for token_ in tokens if token_ == token])\n",
        "        tf_mat[document_index][token_id] = tf\n",
        "        df = sum([1 for document_ in documents if token in document_.split()])\n",
        "        idf = math.log((N+1)/(df + 1)) + 1\n",
        "        #print(f\"token: {token}, tf: {tf}, df: {df}, idf: {idf}, tf_idf: {tf * idf}\")\n",
        "        \n",
        "        \n",
        "        tf_idf[document_index][token_id] = tf * idf\n",
        "    #print(\"*\"*100)\n",
        "\n",
        "tf_idf = normalize(tf_idf)\n",
        "print(f\"Count Matrix: \\n{tf_mat}\\n\")\n",
        "print(f\"TF-IDF Matrix: \\n{tf_idf}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQyvXKlzV28Z",
        "outputId": "2bb13b56-a494-43e2-ee4d-f69ff6cf3ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Matrix: \n",
            "[[0. 1. 1. 1. 0. 0. 1. 0. 1.]\n",
            " [0. 2. 0. 1. 0. 1. 1. 0. 1.]\n",
            " [1. 0. 0. 1. 1. 0. 1. 1. 1.]\n",
            " [0. 1. 1. 1. 0. 0. 1. 0. 1.]]\n",
            "\n",
            "TF-IDF Matrix: \n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF with sklearn\n",
        "\n",
        "*TfidfVectorizer is equilent of applying CountVectorizer and then TfidfTransformer*"
      ],
      "metadata": {
        "id": "3j-_H8jcbxEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TfidfVectorizer"
      ],
      "metadata": {
        "id": "2PUZGHNWcDkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "1HRvzIhaWPmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(smooth_idf=True)\n",
        "tf_idf = vectorizer.fit_transform(documents)"
      ],
      "metadata": {
        "id": "uVb4vrvzWkT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"TF-IDF Matrix: \\n{tf_idf.toarray()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqhQwVgUhVKO",
        "outputId": "8067020b-b5cc-46f3-c366-7cf55cdc6ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix: \n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizer + TfidfTransformer"
      ],
      "metadata": {
        "id": "TzG9rd3OcIU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "gsW8vnNUm66P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([\n",
        "    ('count', CountVectorizer(vocabulary=vocab)),\n",
        "    ('tfid', TfidfTransformer())\n",
        "]).fit(documents)\n",
        "    "
      ],
      "metadata": {
        "id": "0s3_NaoguLJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vector = pipe['count'].transform(documents).toarray()\n",
        "tf_idf = pipe.transform(documents).toarray()"
      ],
      "metadata": {
        "id": "qhldm3h_4An6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Count Matrix: \\n{count_vector}\\n\")\n",
        "print(f\"TF-IDF Matrix: \\n{tf_idf}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6124B7l6RsKi",
        "outputId": "3f63205c-edd0-426d-b9ec-1229a14986a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count Matrix: \n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "\n",
            "TF-IDF Matrix: \n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}